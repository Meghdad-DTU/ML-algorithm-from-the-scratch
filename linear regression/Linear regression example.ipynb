{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from LinearRegression import OLS_Regression, LWLR_Regression, Ridge_Regression, FS_Regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert text file into matrix\n",
    "def file2matrix(filename , header = True, delimeter=\"\\t\", index_y = None):\n",
    "    \"\"\"\n",
    "    Takes a .txt file and returns a list of column names,a matrix of features and\n",
    "    a vectore of y in case of having a target variable.\n",
    "    \"\"\"    \n",
    "    fr = open(filename)\n",
    "    colName = []\n",
    "    if header == True:\n",
    "        colName = list(fr.readline().strip().split(delimeter))\n",
    "    numberOfLines = len(fr.readlines())\n",
    "    fr = open(filename)\n",
    "    numberOfX = len(fr.readline().split(delimeter)) \n",
    "    if index_y is not None:\n",
    "        numberOfX = len(fr.readline().split(delimeter))-1                   \n",
    "    returnMatX = np.zeros((numberOfLines,numberOfX))\n",
    "    classLabelVector = []\n",
    "    fr = open(filename)\n",
    "    firstRow = 0\n",
    "    if header == True:\n",
    "        firstRow = 1\n",
    "    index = 0\n",
    "    for line in fr.readlines()[firstRow:]:\n",
    "        line = line.strip()                            \n",
    "        listFromLine = line.split(delimeter)\n",
    "        if index_y is not None:\n",
    "            classLabelVector.append(float(listFromLine[index_y]))\n",
    "            listFromLine.pop(index_y)\n",
    "        fltListFromLine = list(map(lambda x: float(x) if x!=\"\" else np.nan, listFromLine))                                 \n",
    "        returnMatX[index,:] = fltListFromLine        \n",
    "        index += 1\n",
    "    return colName, returnMatX, classLabelVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy measures \n",
    "def compute_error(trues,predicted):\n",
    "    corr=np.corrcoef(predicted,trues)[0,1]\n",
    "    mae=np.mean(np.abs(predicted-trues))\n",
    "    rae=np.sum(np.abs(predicted-trues))/np.sum(np.abs(trues-np.mean(trues)))\n",
    "    rmse=np.sqrt(np.mean((predicted-trues)**2))\n",
    "    r2=max(0,1-np.sum((trues-predicted)**2)/np.sum((trues-np.mean(trues))**2))\n",
    "    return corr,mae,rae,rmse,r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"abalone.txt\"\n",
    "colName, X, y = file2matrix(filename, header = False, index_y = -1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinary Least Square (OLS) method:** \n",
    "\n",
    "$\\hat w = (X^T . X)^{-1} . X^T . y$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CorrCoef: 0.715\n",
      "MAE: 1.607\n",
      "RMSE: 2.221\n",
      "R2: 0.511\n"
     ]
    }
   ],
   "source": [
    "model = OLS_Regression(fit_intercept=True, normalize=False, method=\"OLS\")\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.prediction(X_test)\n",
    "corr, MAE, RAE, RMSE, R2 = compute_error(y_test,pred)\n",
    "print(\"\\nCorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" %(corr, MAE,RMSE, R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:   0 & loss value: 107.598\n",
      "iteration: 100 & loss value: 6.694\n",
      "iteration: 200 & loss value: 6.447\n",
      "iteration: 300 & loss value: 6.236\n",
      "iteration: 400 & loss value: 6.054\n",
      "iteration: 500 & loss value: 5.898\n",
      "iteration: 600 & loss value: 5.763\n",
      "iteration: 700 & loss value: 5.647\n",
      "\n",
      "CorrCoef: 0.660\n",
      "MAE: 1.776\n",
      "RMSE: 2.485\n",
      "R2: 0.426\n"
     ]
    }
   ],
   "source": [
    "## Using gradient descent\n",
    "model = OLS_Regression(fit_intercept=True, normalize=False, eps=0.001, method=\"GD\", alpha=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.prediction(X_test)\n",
    "corr, MAE, RAE, RMSE, R2 = compute_error(y_test,pred)\n",
    "print(\"\\nCorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" %(corr, MAE,RMSE, R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locally weighted linear regression (LWLR) method:**\n",
    "\n",
    "We give a weight to data points near our data point\n",
    "of interest\n",
    "\n",
    "$\\hat w = (X^T . WX)^{-1} . X^T . Wy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorrCoef: 0.679\n",
      "MAE: 1.622\n",
      "RMSE: 2.366\n",
      "R2: 0.433\n"
     ]
    }
   ],
   "source": [
    "model = LWLR_Regression(kernel=\"guassian\", k=1)\n",
    "pred = model.prediction(X_test, X_train, y_train)\n",
    "corr, MAE, RAE, RMSE, R2 = compute_error(y_test,pred)\n",
    "print(\"CorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" %(corr, MAE,RMSE, R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression:**\n",
    "\n",
    "$\\hat w = (X^T . X + \\lambda I)^{-1} . X^T . y$\n",
    "\n",
    "Ridge regression was originally developed to deal with the problem of having more features than data points. But it can also be used to add bias into our estimations, giving us a better estimate. We can use the $\\lambda$ value to impose a maximum value on the sum of all our $ws$. By imposing this penalty, we can decrease unimportant parameters.\n",
    "This decreasing is known as $shrinkage$ in statistics.\n",
    "\n",
    "**Importnat:** To use ridge regression and all shrinkage methods, you need to first normalize your features.\n",
    "\n",
    "In ridge regression $\\sum_{k=1}^n w_k^2 \\leq \\lambda$ meaning that the sum of the squares of all our weights has to be less than or equal to $\\lambda$ . When two or more of the features are correlated, we may have a very large positive\n",
    "weight and a very large negative weight using regular least-squares regression. By using ridge regression we’re avoiding this problem because the weights are subject to the previous constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficients: [ 0.021  0.024  0.348  0.135  1.469 -1.416 -0.339  0.343]\n",
      "\n",
      "CorrCoef: 0.715\n",
      "MAE: 1.614\n",
      "RMSE: 2.224\n",
      "R2: 0.509\n"
     ]
    }
   ],
   "source": [
    "# Using OLS method for parameter estimation\n",
    "model = Ridge_Regression(fit_intercept=True, normalize= True, method=\"OLS\", lamb = 0.1)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Coeficients:\", np.round(model.coef_,3))\n",
    "pred = model.prediction(X_test)\n",
    "corr, MAE, RAE, RMSE, R2 = compute_error(y_test,pred)\n",
    "print(\"\\nCorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" %(corr, MAE,RMSE, R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:   0 & loss value: 1.069\n",
      "iteration: 100 & loss value: 0.762\n",
      "iteration: 200 & loss value: 0.719\n",
      "iteration: 300 & loss value: 0.707\n",
      "iteration: 400 & loss value: 0.708\n",
      "iteration: 500 & loss value: 0.716\n",
      "iteration: 600 & loss value: 0.725\n",
      "iteration: 700 & loss value: 0.734\n",
      "iteration: 800 & loss value: 0.743\n",
      "iteration: 900 & loss value: 0.750\n",
      "\n",
      "Coeficients: [ 0.014  0.108  0.249  0.154  0.183 -0.771 -0.075  0.757]\n",
      "\n",
      "CorrCoef: 0.705\n",
      "MAE: 1.633\n",
      "RMSE: 2.252\n",
      "R2: 0.497\n"
     ]
    }
   ],
   "source": [
    "# Using Stochastic gradient descent \n",
    "model = Ridge_Regression(fit_intercept=True, normalize= True, method=\"SGD\", lamb = 0.1)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"\\nCoeficients:\", np.round(model.coef_,3))\n",
    "pred = model.prediction(X_test)\n",
    "corr, MAE, RAE, RMSE, R2 = compute_error(y_test,pred)\n",
    "print(\"\\nCorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" %(corr, MAE,RMSE, R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso regression**:\n",
    "\n",
    "The lasso imposes a different constraint on the weights $\\sum_{k=1}^n |w_k| \\leq \\lambda$. The only difference is that we’re taking the absolute value instead of the square of all the weights. To solve this we now need a quadratic programming algorithm. Instead of using the quadratic solver, we can use an easier method for getting results similar to the lasso. This is called **forward stagewise regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward-stagewise algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularize the data to have 0 mean and unit variance\n",
    "# For every iteration:\n",
    "    # Set lowestError to +inf\n",
    "        # For every feature:\n",
    "            # For increasing and decreasing:\n",
    "                # Change one coefficient to get a new W\n",
    "                # Calculate the Error with new W\n",
    "                # If the Error is lower than lowestError: set Wbest to the current W\n",
    "            # Update set W to Wbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficients: [ 0.01 -0.03  0.34  0.14  1.4  -1.39 -0.26  0.37]\n",
      "\n",
      "CorrCoef: 0.729\n",
      "MAE: 1.616\n",
      "RMSE: 2.252\n",
      "R2: 0.528\n"
     ]
    }
   ],
   "source": [
    "model = FS_Regression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Coeficients:\", model.coef_)\n",
    "pred = model.prediction(X_test)\n",
    "corr, MAE, RAE, RMSE, R2 = compute_error(y_test,pred)\n",
    "print(\"\\nCorrCoef: %.3f\\nMAE: %.3f\\nRMSE: %.3f\\nR2: %.3f\" %(corr, MAE,RMSE, R2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
